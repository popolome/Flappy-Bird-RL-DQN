{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1g+wvRxJhSebaf6xPdhnE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/popolome/Flappy-Bird-RL-DQN/blob/main/Flappy_Bird_RL_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Install Dependencies"
      ],
      "metadata": {
        "id": "VfvO8mguurCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfEyk6NiuH07",
        "outputId": "ec5a357a-5163-46a3-efe3-99ac9a0fd946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pygame\n",
        "!pip install tensorflow\n",
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Import libraries"
      ],
      "metadata": {
        "id": "UC9UEy--u2pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pygame\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, optimizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikKsjMfsuwg2",
        "outputId": "212d2389-1b8f-4ec0-81cd-c874e6c62ddc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.6.1 (SDL 2.28.4, Python 3.12.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Set up environment"
      ],
      "metadata": {
        "id": "fBcuORELWL4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FlappyBirdEnv:\n",
        "  def __init__(self):\n",
        "    self.screen_width = 288\n",
        "    self.screen_height = 512\n",
        "    self.pipe_width = 52\n",
        "    self.pipe_gap = 100\n",
        "    self.gravity = 1\n",
        "    self.flap_power = -10\n",
        "    self.bird_y = None\n",
        "    self.bird_vel = None\n",
        "    self.pipe_x = None\n",
        "    self.pipe_y = None\n",
        "    self.done = False\n",
        "\n",
        "  def reset(self):\n",
        "    self.bird_y = self.screen_height / 2\n",
        "    self.bird_vel = 0\n",
        "    self.pipe_x = self.screen_width\n",
        "    self.pipe_y = np.random.randint(50, self.screen_height - 50 - self.pipe_gap)\n",
        "    self.done = False\n",
        "    return self.get_state()\n",
        "\n",
        "  def step(self, action):\n",
        "    if action == 1:     # Action = 0: do nothing, Action = 1: flap\n",
        "      self.bird_vel = self.flap_power\n",
        "    self.bird_vel += self.gravity\n",
        "    self.bird_y += self.bird_vel\n",
        "\n",
        "    self.pipe_x -= 3    # This is the pipe moving speed\n",
        "\n",
        "    if self.pipe_x < -self.screen_width:      # If pipe went off screen, reset it's position\n",
        "      self.pipe_x = self.screen_width\n",
        "      self.pipe_y = np.random.randint(50, self.screen_height - 50 - self.pipe_gap)\n",
        "\n",
        "    if self.bird_y < 0 or self.bird_y > self.screen_height:\n",
        "      self.done = True\n",
        "      reward = -1       # Negative reward if bird went too low or too high screen height\n",
        "    elif (self.pipe_x < 50 and self.pipe_x + self.pipe_width > 0) and \\\n",
        "         (self.bird_y < self.pipe_y or self.bird_y > self.pipe_y + self.pipe_gap):\n",
        "        self.done = True\n",
        "        reward = -1     # Negative reward if bird collide with pipe\n",
        "    else:\n",
        "      reward = 0.1      # Bird gets rewarded for being alive\n",
        "\n",
        "    return self.get_state(), reward, self.done(), {}\n",
        "\n",
        "  def get_state(self):      # Returns 4 dimension states: [bird_y, bird_vel, pipe_dist_x, pipe_dist_y]\n",
        "    pipe_dist_x = self.pipe_x\n",
        "    pipe_dist_y = self.pipe_y + self.pipe_gap/2 - self.bird_y\n",
        "    return np.array([self.bird_y, self.bird_vel, pipe_dist_x, pipe_dist_y], dtype=np.float32)"
      ],
      "metadata": {
        "id": "DCuVk-UBWPk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - DQN Network (Keras)"
      ],
      "metadata": {
        "id": "Km8GdCb-wAmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(state_dim, action_dim):\n",
        "  model = models.Sequential({\n",
        "      layers.Dense(128, activation='relu', input_shape=(state_dim,)),\n",
        "      layers.Dense(128, activation='relu'),\n",
        "      layers.Dense(action_dim, activation='linear')\n",
        "  })\n",
        "  model.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='mse')\n",
        "  return model"
      ],
      "metadata": {
        "id": "fG6-RLGOv8gM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 - Replay Buffer"
      ],
      "metadata": {
        "id": "Hij-2v84yWEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, max_size=10000):   # Set a default max size of 10,000\n",
        "    self.buffer = deque(maxlen=max_size)    # Discard the oldest tuple when more than 10,000\n",
        "\n",
        "  def push(self, state, action, reward, next_state, done):\n",
        "    self.buffer.append((state, action, reward, next_state, done))   # Adds in tuple into self.buffer(which is the deque)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    idx = np.random.choice(len(self.buffer), batch_size, replace=False)   # Randomly pick 32 tuple indices from the self.buffer\n",
        "    states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in idx])    # Breaks them up and group them based on each states, actions, rewards, next_states, and dones\n",
        "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)   # Return them as arrays\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)   # Return the len of the self.buffer if the ReplayBuffer len is called"
      ],
      "metadata": {
        "id": "PDjqYYKtyU__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 - DQN Agent"
      ],
      "metadata": {
        "id": "Z5XWE8RFDaj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "  def __init__(self, state_dim, action_dim, gamma=0.99, epsilon=1.0, min_epsilon=0.05, epsilon_decay=0.995):\n",
        "    self.state_dim = state_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.min_epsilon = min_epsilon\n",
        "    self.epsilon_decay = epsilon_decay    # Making variables store in instance of class DQNAgent to be used by other methods\n",
        "\n",
        "    self.model = build_model(state_dim, action_dim)\n",
        "    self.memory = ReplayBuffer()\n",
        "\n",
        "  def select_action(self, state):\n",
        "    if np.random.rand() < self.epsilon:\n",
        "      return np.random.randint(self.action_dim)   # Take a random action if less then epsilon value\n",
        "    q_values = self.model.predict(state[np.newaxis, :], verbose=0)    # Converts the 1D array into 2D array with batch_size and state\n",
        "    return np.argmax(q_values[0])   # Else exploit the highest q-value\n",
        "\n",
        "  def train(self, batch_size=64):     # Temporary stops training first if self.buffer less than batch_size\n",
        "    if len(self.memory) < batch_size:\n",
        "      return\n",
        "\n",
        "    states, actions, rewards, next_states, dones = self.memory.sample(batch_size)   # Randomly pick complete experiences and returns batches of states, actions, rewards, next_states, dones\n",
        "\n",
        "    q_next = self.model.predict(next_states, verbose=0)   # Predict future Q-values\n",
        "    q_target = self.model.predict(states, verbose=0)    # Predict Q-values\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      q_target[i, actions[i]] = rewards[i] + self.gamma * np.max(q_next[i]) * (1 - dones[i])    # The bellman equation from 0 to batch_size\n",
        "\n",
        "    self.model.fit(states, q_target, epochs=1, verbose=0)   # Adjust the network based on single batch\n",
        "\n",
        "    self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)   # Slowly transition from exploration to exploitation"
      ],
      "metadata": {
        "id": "WuMCIBCIDdbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 - Training Loop"
      ],
      "metadata": {
        "id": "Mbyc7cauRNWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = FlappyBirdEnv()     # Create the environment\n",
        "state_dim = 4\n",
        "action_dim = 2\n",
        "\n",
        "num_episodes = 5000\n",
        "agent = DQNAgent(state_dim=state_dim, action_dim=action_dim)\n",
        "\n",
        "reward_history = []     # For monitoring the model progress\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  total_rewards = 0\n",
        "\n",
        "  while not done:\n",
        "    action = agent.select_action(state)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "\n",
        "    agent.memory.push(state, action, reward, next_state, done)\n",
        "    agent.train()\n",
        "\n",
        "    state = next_state\n",
        "    total_rewards += reward\n",
        "\n",
        "  reward_history.append(total_rewards)      # Append the total_rewards into monitoring list\n",
        "\n",
        "  if (episode + 1) % 500 == 0:\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_rewards}, Epsilon: {agent.epsilon:.3f}\")"
      ],
      "metadata": {
        "id": "a-3-BQr_RQqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 - Monitoring Running Average"
      ],
      "metadata": {
        "id": "Tx3zeKFWgfQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot([np.mean(reward_history[i:i+100]) for i in range(len(reward_history)-100)])\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Running Average Reward\")\n",
        "plt.title(\"Flappy Bird DQN Training Progress\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "gOy_vcifgnND",
        "outputId": "626ab0d5-f7a9-42e6-d468-a1174f9deb8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'reward_history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-137434847.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episodes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running Average Reward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Flappy Bird DQN Training \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'reward_history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 - Save the Trained Model"
      ],
      "metadata": {
        "id": "a1FDOX9zhOPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.model.save(\"flappy_dqn_model.h5\")"
      ],
      "metadata": {
        "id": "GmpECYLzhFHZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}