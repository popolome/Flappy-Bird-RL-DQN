{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497a82dd-17e5-4296-aacc-5e846853e4a3",
   "metadata": {},
   "source": [
    "# My First Flappy Bird DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550362a-12ab-4b10-94d2-ce136e281f97",
   "metadata": {},
   "source": [
    "# Imports and device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89891395-ae33-42ec-80d7-3c15fbe6183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flappy_bird_gymnasium\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "import itertools\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# For printing date and time\n",
    "DATE_FORMAT = \"%m-%d %H:%M:%S\"\n",
    "\n",
    "RUN_DIR = \"runs\"\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "yaml_text = \"\"\"\n",
    "cartpole1:\n",
    "    env_id: CartPole-v1\n",
    "    replay_memory_size: 100000\n",
    "    mini_batch_size: 32\n",
    "    epsilon_init: 1\n",
    "    epsilon_decay: 0.99995\n",
    "    min_epsilon: 0.05\n",
    "    network_sync_rate: 10\n",
    "    learning_rate_a: 0.002\n",
    "    discount_factor_g: 0.99\n",
    "    stop_on_reward: 4000\n",
    "    fc1_nodes: 64\n",
    "\n",
    "flappybird1:\n",
    "    env_id: FlappyBird-v0\n",
    "    replay_memory_size: 100000\n",
    "    mini_batch_size: 64\n",
    "    epsilon_init: 1.0\n",
    "    epsilon_decay: 0.99995\n",
    "    min_epsilon: 0.05\n",
    "    network_sync_rate: 10\n",
    "    learning_rate_a: 0.00001\n",
    "    discount_factor_g: 0.99\n",
    "    stop_on_reward: 2000\n",
    "    fc1_nodes: 512\n",
    "    env_make_params:\n",
    "        use_lidar: false\n",
    "\"\"\"\n",
    "\n",
    "all_hyperparameter_sets = yaml.safe_load(yaml_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847dca1-a57f-4ce4-af50-394f11b11afb",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82993648-f85f-448e-83f9-727b144f01cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'load_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 241\u001b[0m\n\u001b[1;32m    235\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflappybird1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# To train the agent\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# agent.run(is_training=True)\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# To test the trained agent\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_dqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m(torch\u001b[38;5;241m.\u001b[39mload(agent\u001b[38;5;241m.\u001b[39mMODEL_FILE))\n\u001b[1;32m    242\u001b[0m agent\u001b[38;5;241m.\u001b[39mpolicy_dqn\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Switch to eval mode\u001b[39;00m\n\u001b[1;32m    243\u001b[0m agent\u001b[38;5;241m.\u001b[39mrun(is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'load_state_dict'"
     ]
    }
   ],
   "source": [
    "# DQN Network\n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "\n",
    "# Replay Memory\n",
    "class ReplayMemory:\n",
    "    def __init__(self, maxlen, seed=None):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "# Agent\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, hyperparameter_set):\n",
    "        hyperparameter = all_hyperparameter_sets[hyperparameter_set]\n",
    "\n",
    "        # Attributes\n",
    "        self.hyperparameter_set = hyperparameter_set\n",
    "        self.env_id = hyperparameter['env_id']\n",
    "        self.replay_memory_size = hyperparameter['replay_memory_size']\n",
    "        self.mini_batch_size = hyperparameter['mini_batch_size']\n",
    "        self.epsilon_init = hyperparameter['epsilon_init']\n",
    "        self.epsilon_decay = hyperparameter['epsilon_decay']\n",
    "        self.min_epsilon = hyperparameter['min_epsilon']\n",
    "        self.network_sync_rate = hyperparameter['network_sync_rate']\n",
    "        self.learning_rate_a = hyperparameter['learning_rate_a']\n",
    "        self.discount_factor_g = hyperparameter['discount_factor_g']\n",
    "        self.stop_on_reward = hyperparameter['stop_on_reward']\n",
    "        self.fc1_nodes = hyperparameter['fc1_nodes']\n",
    "        self.env_make_params = hyperparameter['env_make_params']\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()    # NN loss function using the MSE-Mean Squared Error\n",
    "        self.optimizer = None    # Initialize optimizer to None for now\n",
    "\n",
    "        self.rewards_per_episode = []\n",
    "        self.epsilon_history = []\n",
    "\n",
    "        # Path to Run info\n",
    "        self.LOG_FILE = os.path.join(RUN_DIR, f'{self.hyperparameter_set}.log')\n",
    "        self.MODEL_FILE = os.path.join(RUN_DIR, f'{self.hyperparameter_set}.pt')\n",
    "        self.GRAPH_FILE = os.path.join(RUN_DIR, f'{self.hyperparameter_set}.png')\n",
    "\n",
    "        self.policy_dqn = None\n",
    "    \n",
    "    def run (self, is_training=False, render=False):\n",
    "        #env = gym.make('FlappyBird-v0', render_mode='human' if render else None, use_lidar=False)\n",
    "        if is_training:\n",
    "            env = gym.make('FlappyBird-v0', render_mode='human' if render else None, use_lidar=self.env_make_params.get('use_lidar', False))\n",
    "        else:\n",
    "            env = gym.make('FlappyBird-v0', render_mode='human' if render else None, use_lidar=False)\n",
    "\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        self.policy_dqn = DQN(num_states, num_actions, self.fc1_nodes).to(device)\n",
    "\n",
    "        # Initialize ReplayMemory if training\n",
    "        if is_training:\n",
    "            memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "            epsilon = self.epsilon_init\n",
    "\n",
    "            target_dqn = DQN(num_states, num_actions, self.fc1_nodes).to(device)\n",
    "            target_dqn.load_state_dict(self.policy_dqn.state_dict())\n",
    "\n",
    "            step_count = 0    # Initialize a step counter\n",
    "\n",
    "            # Optimizer using Adam to update the policy network parameters using defined learning rate\n",
    "            self.optimizer = torch.optim.Adam(self.policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "\n",
    "            # Track best reward for training\n",
    "            best_reward = float('-inf')\n",
    "        else:\n",
    "            # Load learned policy\n",
    "            self.policy_dqn.load_state_dict(torch.load(self.MODEL_FILE))\n",
    "\n",
    "            # Switch model to evaluation mode\n",
    "            self.policy_dqn.eval()\n",
    "\n",
    "            epsilon = self.epsilon_init   # Keep epsilon initialized even after training\n",
    "\n",
    "            # Track best reward for evaluation\n",
    "            best_reward = float('-inf')\n",
    "\n",
    "\n",
    "        # Run the game indefinitely\n",
    "        for episode in itertools.count():     \n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float, device=device)\n",
    "            \n",
    "            terminated = False\n",
    "            episode_reward = 0.0\n",
    "            \n",
    "            while not terminated:                \n",
    "                if is_training and random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                    action = torch.tensor(action, dtype=torch.int64, device=device)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        # Converts the 1D tensor into 2D and then convert it back to 1D, then output is the highest action index\n",
    "                        action = self.policy_dqn(state.unsqueeze(dim=0)).squeeze().argmax()\n",
    "    \n",
    "                # Processing\n",
    "                new_state, reward, terminated, _, info = env.step(action.item())\n",
    "\n",
    "                # Accumulate reward\n",
    "                episode_reward += reward\n",
    "\n",
    "                # Convert new state and reward to tensors in device\n",
    "                new_state = torch.tensor(new_state, dtype=torch.float, device=device)\n",
    "                reward = torch.tensor(reward, dtype=torch.float, device=device)\n",
    "    \n",
    "                if is_training:\n",
    "                    memory.append((state, action, new_state, reward, terminated))\n",
    "\n",
    "                    step_count += 1    # Increment step counter\n",
    "                \n",
    "                # Move to new state\n",
    "                state = new_state\n",
    "\n",
    "\n",
    "            if is_training:\n",
    "                if episode_reward > best_reward:\n",
    "                    log_message = f\"{datetime.now().strftime(DATE_FORMAT)}: New best reward: {episode_reward:0.1f} ({(episode_reward-best_reward)})\"\n",
    "                    print(log_message)\n",
    "                    print(f'---Episode {episode}, Reward: {episode_reward: .1f}, Epsilon: {epsilon: .4f}')\n",
    "                    with open(self.LOG_FILE, 'a') as file:\n",
    "                        file.write(log_message + '\\n')\n",
    "\n",
    "                    torch.save(self.policy_dqn.state_dict(), self.MODEL_FILE)\n",
    "                    best_reward = episode_reward\n",
    "\n",
    "            if is_training:\n",
    "                epsilon = max(epsilon * self.epsilon_decay, self.min_epsilon)\n",
    "\n",
    "            self.rewards_per_episode.append(episode_reward)\n",
    "            self.epsilon_history.append(epsilon)\n",
    "\n",
    "            if is_training and len(memory) > self.mini_batch_size:\n",
    "                \n",
    "                mini_batch = memory.sample(self.mini_batch_size)    # Sample from memory\n",
    "                \n",
    "                self.optimize(mini_batch, self.policy_dqn, target_dqn)\n",
    "\n",
    "                # Update the policy target with the policy network after a certain number of steps\n",
    "                if step_count >= self.network_sync_rate:\n",
    "                    target_dqn.load_state_dict(self.policy_dqn.state_dict())\n",
    "                    step_count = 0\n",
    "\n",
    "            # Stop training when reward is more or equal to stopping reward\n",
    "            if is_training and best_reward >= self.stop_on_reward:\n",
    "                print(f\"Best reward reached: {best_reward}. Stopping training.\")\n",
    "                break\n",
    "\n",
    "            if not is_training:\n",
    "                break\n",
    "\n",
    "    \n",
    "    def save_graph(self, rewards_per_episode, epsilon_history):\n",
    "        # Save plots\n",
    "        fig = plt.figure(1)\n",
    "\n",
    "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
    "        mean_rewards = np.zeros(len(rewards_per_episode))\n",
    "        for x in range(len(mean_rewards)):\n",
    "            mean_rewards[x] = np.mean(rewards_per_episode[max(0, x-99):(x+1)])\n",
    "        plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
    "        # plt.xlabel('Episodes')\n",
    "        plt.ylabel('Mean Rewards')\n",
    "        plt.plot(mean_rewards)\n",
    "\n",
    "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
    "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
    "        # plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Epsilon Decay')\n",
    "        plt.plot(epsilon_history)\n",
    "\n",
    "        plt.subplots_adjust(wspace=1.0, hspace=1.0)\n",
    "\n",
    "        # Save plots\n",
    "        fig.savefig(self.GRAPH_FILE)\n",
    "        plt.close(fig)\n",
    "        \n",
    "                    \n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "        states, actions, new_states, rewards, terminations = zip(*mini_batch)    # Transpose the list of exp and separate each element\n",
    "\n",
    "        states = torch.stack(states)    # Stack tensors into batch tensors, ([[1, 4], [2, 4]])\n",
    "\n",
    "        actions = torch.stack(actions)\n",
    "\n",
    "        new_states = torch.stack(new_states)\n",
    "\n",
    "        rewards = torch.stack(rewards)\n",
    "        terminations = torch.tensor(terminations).float().to(device)    # Convert terminations to tensor and float, to GPU\n",
    "\n",
    "        # Calculate the Q-values with Bellman and no gradient updates\n",
    "        with torch.no_grad():\n",
    "            target_q = rewards + (1 - terminations) * self.discount_factor_g * target_dqn(new_states).max(dim=1)[0]\n",
    "        \n",
    "        current_q = policy_dqn(states).gather(dim=1, index=actions.unsqueeze(dim=1)).squeeze()    # Take actions and return q-values\n",
    "\n",
    "        loss = self.loss_fn(current_q, target_q)    # Get the loss from current and target q-values\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()    # Remove the previous gradients from the previous step\n",
    "        loss.backward()    # Get the weights\n",
    "        self.optimizer.step()    # Reduce the loss so the next current_q is closer to the target_q\n",
    "\n",
    "# Create the agent\n",
    "agent = Agent('flappybird1')\n",
    "\n",
    "# To train the agent, uncomment this out if you want to re-train the whole model\n",
    "# agent.run(is_training=True)\n",
    "\n",
    "# To test the trained agent\n",
    "agent.policy_dqn.load_state_dict(torch.load(agent.MODEL_FILE))\n",
    "agent.policy_dqn.eval()  # Switch to eval mode\n",
    "agent.run(is_training=False, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592bcfaf-6067-4bad-869c-d88068ce61db",
   "metadata": {},
   "source": [
    "# Main execution / testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e4dff93-2bb9-4846-981b-b5d1adb45bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5208/919182949.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.policy_dqn.load_state_dict(torch.load(agent.MODEL_FILE))\n",
      "/tmp/ipykernel_5208/3809470938.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.policy_dqn.load_state_dict(torch.load(self.MODEL_FILE))\n"
     ]
    }
   ],
   "source": [
    "# To test the trained agent\n",
    "agent.policy_dqn.load_state_dict(torch.load(agent.MODEL_FILE))\n",
    "agent.policy_dqn.eval()  # Switch to eval mode\n",
    "agent.run(is_training=False, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e10f971-d080-4e94-9e40-d2348bd5fce6",
   "metadata": {},
   "source": [
    "# Play Flappy Bird Game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdfcc672-1ecf-4de7-ab20-36ddff6a528b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_508/4093140758.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.policy_dqn.load_state_dict(torch.load(self.MODEL_FILE))\n"
     ]
    }
   ],
   "source": [
    "agent = Agent('flappybird1')\n",
    "\n",
    "agent.run(is_training=False, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow with GPU",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
